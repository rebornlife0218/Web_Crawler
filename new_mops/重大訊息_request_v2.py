import requests
import time
import pandas as pd
import calendar
from selenium import webdriver
from datetime import datetime
import json
import os
import random

# å‹•æ…‹ç²å– Cookieï¼ˆå«å¿«å–ï¼‰
def get_cookies():
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)
    driver.get("https://mops.twse.com.tw/mops/#/web/t05st02")
    time.sleep(2)
    cookies = driver.get_cookies()
    requests_cookies = {cookie["name"]: cookie["value"] for cookie in cookies}
    driver.quit()
    return requests_cookies

def get_cookies_cached():
    cache_path = "mops_cookie.json"
    if os.path.exists(cache_path):
        try:
            with open(cache_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    cookies = get_cookies()
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(cookies, f)
    return cookies

# æ—¥æœŸæ¨¡å¼è™•ç†
def get_date_range(mode, start_year=None):
    today = datetime.today()
    if mode == "today":
        return [(today.year - 1911, today.month, today.day)]
    elif mode == "from_year" and start_year is not None:
        dates = []
        for year in range(start_year, today.year - 1911 + 1):
            for month in range(1, 13):
                if year == today.year - 1911 and month > today.month:
                    break
                days_in_month = calendar.monthrange(year + 1911, month)[1]
                for day in range(1, days_in_month + 1):
                    if year == today.year - 1911 and month == today.month and day > today.day:
                        break
                    dates.append((year, month, day))
        return dates
    else:
        raise ValueError("Invalid mode or missing start_year")

# çˆ¬èŸ²
def scrape_mops_data(mode="today", start_year=None):
    start_time = time.time()
    all_data = []
    error_log = []
    base_url = "https://mops.twse.com.tw/mops/api/t05st02"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Content-Type": "application/json",
        "Accept": "application/json"
    }
    cookies = get_cookies_cached()
    date_range = get_date_range(mode, start_year)

    for year, month, day in date_range:
        retry = 0
        max_retry = 3
        print(f"\nğŸ“… æ­£åœ¨çˆ¬å–ï¼š{year}-{month:02d}-{day:02d}")

        while retry < max_retry:
            stage_times = {}
            daily_data = []

            try:
                # ç™¼é€ POST è«‹æ±‚
                stage_start = time.time()
                payload = {
                    "year": str(year),
                    "month": str(month),
                    "day": str(day).zfill(2),
                    "TYPEK": "all",
                    "encodeURIComponent": 1
                }
                response = requests.post(
                    base_url,
                    json=payload,
                    headers=headers,
                    cookies=cookies,
                    timeout=20
                )
                stage_times["API è«‹æ±‚"] = time.time() - stage_start
                
                # è™•ç†éŸ¿æ‡‰
                if response.status_code == 200:
                    try:
                        stage_start = time.time()
                        data = response.json()
                        if data.get("code") != 200:
                            raise ValueError(f"API å›å‚³ code éŒ¯èª¤ï¼š{data.get('code')}")

                        for item in data.get("result", {}).get("data", []):
                            if len(item) >= 5 and "è‘£äº‹æœƒæ±ºè­°ç™¼è¡Œ" in item[4] and "è½‰æ›å…¬å¸å‚µ" in item[4]:
                                daily_data.append({
                                    "ç™¼è¨€æ—¥æœŸ": item[0],
                                    "ç™¼è¨€æ™‚é–“": item[1],
                                    "å…¬å¸ä»£è™Ÿ": item[2],
                                    "å…¬å¸åç¨±": item[3],
                                    "ä¸»æ—¨": item[4]
                                })
                        stage_times["JSON è™•ç†"] = time.time() - stage_start
                        print("âœ… ç•¶æ—¥çˆ¬å–ç­†æ•¸ï¼š", len(daily_data))
                        all_data.extend(daily_data)

                        if not daily_data:
                            print("âŒ ç„¡ç¬¦åˆè³‡æ–™")
                        break  # ç„¡è«–æœ‰ç„¡è³‡æ–™çš†è·³å‡º retry

                    except Exception as e:
                        retry += 1
                        print(f"âŒ JSON è§£æéŒ¯èª¤ï¼Œé‡è©¦ä¸­...ï¼š{e}")
                        time.sleep(2)
                else:
                    retry += 1
                    print(f"âš ï¸ HTTP ç‹€æ…‹ç¢¼éŒ¯èª¤ï¼š{response.status_code}ï¼Œé‡è©¦ä¸­...")
                    if response.status_code == 502:
                        print("ğŸ” æª¢æ¸¬åˆ° 502 Bad Gatewayï¼Œå¯èƒ½æ˜¯ä¼ºæœå™¨æš«æ™‚ä¸å¯ç”¨")
                    time.sleep(random.uniform(3, 6))

            except requests.exceptions.ReadTimeout:
                print(f"âŒ ReadTimeoutï¼š{year}-{month:02d}-{day:02d} => ä¼ºæœå™¨å·²é€£ç·šä½†è®€å–è¶…æ™‚")
                retry += 1
                time.sleep(random.uniform(3, 6))
                if retry == max_retry:
                    error_log.append({
                        "æ—¥æœŸ": f"{year}-{month:02d}-{day:02d}",
                        "éŒ¯èª¤è¨Šæ¯": "ReadTimeout"
                    })

            except requests.exceptions.ConnectTimeout:
                print(f"âŒ ConnectTimeoutï¼š{year}-{month:02d}-{day:02d} => ç„¡æ³•é€£ä¸Šä¼ºæœå™¨ï¼ˆé€£ç·šè¶…æ™‚ï¼‰")
                retry += 1
                time.sleep(random.uniform(3, 6))
                if retry == max_retry:
                    error_log.append({
                        "æ—¥æœŸ": f"{year}-{month:02d}-{day:02d}",
                        "éŒ¯èª¤è¨Šæ¯": "ConnectTimeout"
                    })

            except requests.exceptions.ConnectionError as ce:
                print(f"âŒ ConnectionErrorï¼š{year}-{month:02d}-{day:02d} => {ce}")
                retry += 1
                time.sleep(random.uniform(3, 6))
                if retry == max_retry:
                    error_log.append({
                        "æ—¥æœŸ": f"{year}-{month:02d}-{day:02d}",
                        "éŒ¯èª¤è¨Šæ¯": f"ConnectionError: {ce}"
                    })

            except Exception as e:
                print(f"âŒ å…¶ä»–éŒ¯èª¤ï¼š{year}-{month:02d}-{day:02d} => {e}")
                retry += 1
                time.sleep(random.uniform(3, 6))
                if retry == max_retry:
                    error_log.append({
                        "æ—¥æœŸ": f"{year}-{month:02d}-{day:02d}",
                        "éŒ¯èª¤è¨Šæ¯": str(e)
                    })

        # è¼¸å‡ºéšæ®µæ™‚é–“
        print("å„éšæ®µè€—æ™‚ï¼š")
        for stage, t in stage_times.items():
            print(f"  {stage}: {t:.2f} ç§’")

        time.sleep(0.5) # åçˆ¬èŸ²

    if error_log:
        print("\nâ— ä»¥ä¸‹æ—¥æœŸç™¼ç”ŸéŒ¯èª¤ï¼š")
        for entry in error_log:
            print(f"  ğŸ“… {entry['æ—¥æœŸ']} => âŒ {entry['éŒ¯èª¤è¨Šæ¯']}")
    else:
        print("\nâœ… æ‰€æœ‰æ—¥æœŸçš†æˆåŠŸçˆ¬å–")

    end_time = time.time()
    print(f"\nâœ… ç¸½èŠ±è²»æ™‚é–“ï¼š{end_time - start_time:.2f} ç§’ï¼Œç¸½ç­†æ•¸ï¼š{len(all_data)}")

    # ç™¼è¨€æ—¥æœŸè½‰ç‚º datetime ç‰©ä»¶ï¼Œä¸¦ä¾ä¸»æ—¨å»é™¤é‡è¤‡ï¼Œä¿ç•™æœ€æ—©æ—¥æœŸ
    df = pd.DataFrame(all_data)
    df["ç™¼è¨€æ—¥æœŸ"] = df["ç™¼è¨€æ—¥æœŸ"].str.replace("/", "-")
    df["ç™¼è¨€æ—¥æœŸ"] = df["ç™¼è¨€æ—¥æœŸ"].apply(lambda x: f"{1911 + int(x.split('-')[0])}-{x.split('-')[1]}-{x.split('-')[2]}")
    df["ç™¼è¨€æ—¥æœŸ"] = pd.to_datetime(df["ç™¼è¨€æ—¥æœŸ"], errors="coerce")
    df = df.sort_values("ç™¼è¨€æ—¥æœŸ").drop_duplicates(subset="ä¸»æ—¨", keep="first")
    df.to_csv("é‡å¤§è¨Šæ¯.csv", index=False, encoding="utf-8-sig")
    print("âœ… è³‡æ–™å·²å„²å­˜ç‚ºï¼šé‡å¤§è¨Šæ¯_v3.csvï¼Œç¸½ç­†æ•¸ï¼š", len(df))

# Case1: å–®ç¨çˆ¬å–ä»Šå¤©è³‡æ–™
# scrape_mops_data(mode="today")

# Case2: çˆ¬å–å¾æŒ‡å®šå¹´ä»½åˆ°ä»Šå¤©
scrape_mops_data(mode="from_year", start_year=108)